[project]
name = "llama-stack-runner"
version = "0.1.0"
description = "Llama Stack runner"
authors = []
dependencies = [
    "fastapi>=0.115.6",
    "uvicorn>=0.34.3",
    "kubernetes>=30.1.0",
    "llama-stack==0.2.18",
    "llama-stack-client==0.2.18",
    "ollama>=0.2.0",
    "openai==1.99.9",
    "rich>=14.0.0",
    "cachetools>=6.1.0",
    "prometheus-client>=0.22.1",
    "starlette>=0.47.1",
    "aiohttp>=3.12.14",
    "authlib>=1.6.0",
    "opentelemetry-sdk>=1.34.0",
    "opentelemetry-exporter-otlp>=1.34.0",
    "opentelemetry-instrumentation>=0.55b0",
    "aiosqlite>=0.21.0",
    "litellm>=1.72.1",
    "blobfile>=3.0.0",
    "datasets>=3.6.0",
    "sqlalchemy>=2.0.41",
    "faiss-cpu>=1.11.0",
    "mcp>=1.9.4",
    "autoevals>=0.0.129",
    "psutil>=7.0.0",
    "peft>=0.15.2",
    "trl>=0.18.2",
    "greenlet",
    "torch",
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}

[[tool.uv.index]]
name = "pypi"
url = "https://pypi.org/simple"

[[tool.uv.index]]
name = "torch-cpu"
url = "https://download.pytorch.org/whl/cpu"

[tool.uv.sources]
torch = { index = "torch-cpu" }

[tool.uv]
override-dependencies = [
  "nvidia-cublas-cu12 ; sys_platform=='never'",
  "nvidia-cuda-cupti-cu12 ; sys_platform=='never'",
  "nvidia-cuda-nvrtc-cu12 ; sys_platform=='never'",
  "nvidia-cuda-runtime-cu12 ; sys_platform=='never'",
  "nvidia-cudnn-cu12 ; sys_platform=='never'",
  "nvidia-cufft-cu12 ; sys_platform=='never'",
  "nvidia-curand-cu12 ; sys_platform=='never'",
  "nvidia-cusolver-cu12 ; sys_platform=='never'",
  "nvidia-cusparse-cu12 ; sys_platform=='never'",
  "nvidia-nccl-cu12 ; sys_platform=='never'",
  "nvidia-nvtx-cu12 ; sys_platform=='never'",
  "nvidia-nvjitlink-cu12 ; sys_platform=='never'",
  "nvidia-cusparselt-cu12 ; sys_platform=='never'",
  "nvidia-cufile-cu12 ; sys_platform=='never'",
  "triton ; sys_platform=='never'",
]

[tool.pdm]
distribution = false
