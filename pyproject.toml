[project]
name = "llama-stack-runner"
version = "0.1.0"
description = "Llama Stack runner"
authors = []
dependencies = [
    "fastapi>=0.115.6",
    "uvicorn>=0.34.3",
    "kubernetes>=30.1.0",
    "llama-stack==0.2.17",
    "llama-stack-client==0.2.17",
    "openai>=1.66,<1.85",
    "rich>=14.0.0",
    "cachetools>=6.1.0",
    "prometheus-client>=0.22.1",
    "starlette>=0.47.1",
    "aiohttp>=3.12.14",
    "authlib>=1.6.0",
    "opentelemetry-sdk>=1.34.0",
    "opentelemetry-exporter-otlp>=1.34.0",
    "opentelemetry-instrumentation>=0.55b0",
    "aiosqlite>=0.21.0",
    "litellm>=1.72.1",
    "blobfile>=3.0.0",
    "datasets>=3.6.0",
    "sqlalchemy>=2.0.41",
    "faiss-cpu>=1.11.0",
    "mcp>=1.9.4",
    "autoevals>=0.0.129",
    "psutil>=7.0.0",
    "peft>=0.15.2",
    "trl>=0.18.2",
    "greenlet",
    "torch",
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}

[tool.uv.sources]
pytorch-cpu = "https://download.pytorch.org/whl/cpu"

[tool.pdm]
distribution = false
